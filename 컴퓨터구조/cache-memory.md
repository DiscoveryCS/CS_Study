## 캐시 메모리

## Contents
1. 캐시 메모리란?
2. 캐시 메모리의 주요 특징
3. 캐시 메모리 동작 원리
   1. LRU Cache
4. 캐시 메모리 접근 정책
5. 캐시 메모리 쓰기 정책

## 1. 캐시 메모리란?

- CPU와 주기억장치(RAM) 사이에 위치한 고속 임시 저장 장치

  - 사용하는 이유: CPU와 주기억장치 속도 차이를 보완하기 위함. CPU가 프로그램 코드나 데이터를 인출하는 과정에 있어 주기억장치에 접근해야 한다.
  하지만 CPU와 주기억장치 간의 데이터 조회는 긴 시간이 소요되므로 성능 저하를 방지하기 위해 캐시메모리를 설치한다.

## 2. 캐시 메모리의 주요 특징

- 빠른 접근 속도

  - 캐시 메모리는 메인 메모리보다 훨씬 빠른 속도를 제공한다. 이를 통해 CPU가 데이터를 더 빨리 읽고 쓸 수 있다.

- 작은 용량

  - 캐시 메모리는 물리적 크기와 비용 문제로 인해 메인 메모리보다 용량이 훨씬 작다.

## 3. 캐시 메모리의 동작 원리

1. CPU가 기억장치로부터 어떠한 데이터를 읽으려고 할 때, 먼저 캐시에 데이터가 있는지 검사한다.
2. 만약 데이터가 캐시에 있다면 데이터를 캐시에서 꺼내온 후, CPU에게 전달한다.
   1. 이 상태를 캐시 적중(Cache Hit)라고 한다.
   2. 캐시 적중은 데이터의 지역성(locality)에 의존한다.
      1. 지역성: 컴퓨터 프로그래밍이 데이터를 접근할 때, 특정 데이터나 그 데이터의 가까운 주변에 대해 반복적으로 접근하는 경향을 의미한다.
      2. 시간적 지역성
         1. 최근 엑세스된 데이터가 다시 액세스될 가능성이 높아지는 현상
         2. ex. 반복문의 변수
      3. 공간적 지역성
         1. 기억장치 내에 서로 인접하여 저장되어 있는 데이터들이 연속적으로 액세스될 가능성이 높아지는 현상
         2. ex. 배열의 메모리 공간
      4. 순차적 지역성
         1. 분기(branch)가 발생하지 않는 한, 기억장치에 저장된 순서대로 인출되어 실행되는 순서
         2. ex. 배열의 인덱스
3. 만약 데이터가 캐시에 없다면 주기억장치에서 데이터를 꺼내온다. 이때 데이터를 가져오면서 캐시에도 해당 데이터를 복사 및 갱신한다.
   1. 이 상태를 캐시 미스(Cache Miss)라고 한다.
   2. 캐시 미스(Cache Miss)는 왜 발생할까?
      1. 캐시 메모리의 크기가 작아서일 수 있다. 캐시 메모리의 크기가 작은 경우 적은 양의 데이터만 저장할 수 있기 때문에 캐시 미스의 가능성이 높아진다.
      2. 캐시 교체 알고리즘의 문제일 수 있다. 캐시 교체 알고리즘이 불필요하게 데이털르 지우는 경우에 캐시 미스의 가능성이 높아진다.
4. 캐시 공간이 작으므로, 공간이 모자라게 되면 안쓰는 데이터부터 삭제하여 공간을 확보한다.

### i. LRU Cache

- LRU(Least Recently Used)

  - 운영체제의 페이지 교체 알고리즘 중 하나이다.
  - 페이지를 교체할 때 가장 오랫동안 사용되지 않은 페이지를 교체 대상으로 삼는 기법이다.

- LRU Cache

  - 캐시에 공간이 부족할 때 가장 오랫동안 사용하지 않는 항목을 제거하고 새로운 요소를 배치하는 방식
  - 오랫동안 사용되지 않은 항목은 앞으로도 사용되지 않을 가능성이 많기 때문에 가장 오랫동안 참조되지 않은 요소를 제거하자 라는 의미이다.

- 호출 방식

  - size는 3이고, 1 → 2 → 3 순차 호출을 하게 된다면 아래와 같다.
    - 3 → 2 → 1
  - 2번 캐시를 호출한다.
    - 2 → 3 → 1
  - 여기서 데이터 4를 새로 캐시에 쓰게 되면, Least Recently Used인 1번 항목은 제거된다.
    - 4 → 2 → 3

## 4. 캐시 메모리의 접근 정책 (캐시 매핑 기법)

```markdown
캐시의 크기는 메모리보다 작기 때문에 효율적으로 메모리의 데이터를 매핑하는 것이 중요하다.
```

### 직접 매핑 기법(Direct Mapping)

- 메모리의 특정 블록(block)을 특정 캐시에만 매핑하는 것을 의미한다.
- 장점
  - 구현이 단순하다.
- 단점
  - 교체가 자주 일어나기 때문에 Cache Hit가 낮다.
  - 특정 동일한 캐시 블록에 매핑되는 다른 메모리 블록을 번갈아 참조하게 되면 블록 충돌이 발생하여 Cache Hit가 발생하지 않는다.
    - ex. 20~30에 해당하는 값을 로드해서 사용해야 하는데, 이를 저장할 캐시 공간은 2만 있으므로 매번 캐시 교체가 일어난다.

### 완전 연관 매핑(Full Associate Mapping)

- 메모리의 어떤 블록이라도 캐시 메모리의 어떤 위치에 저장될 수 있는 것을 의미한다.
- 장점
  - 충돌이 거의 없다.
- 단점
  - 찾는 과정이 직접 매핑에 비해서 오래 걸린다.

### 집합 연관 매핑(Set Associate Mapping)

- 직접 매핑(1)과 연관 매핑(2)를 더한 방식
- 캐시를 여러 개의 집합으로 나누고, 각 집합은 여러 개의 캐시 라인으로 구성되는 것을 의미한다.
- 예를 들어, 1~50까지의 메모리 주소가 있고, 캐시가 1-5까지 있따면 1~20까지의 데이터는 1~2에, 20~50까지의 데이터는 2-5에 무작위로 저장하는 방식
- 장점
  - 블록화가 되어 있어 검색은 효율적이다.
  - 직접 매핑처럼 저장위치에 제약이 있지 않으므로 적중률도 직접 매핑에 비해서는 좋다.

## 5. 캐시 메모리 쓰기 정책

### 즉시 쓰기(Write Through)

- 캐시 메모리에 데이터를 쓰는 시점에 데이터를 주기억장치에도 저장하는 정책을 의미한다.
- 캐시 메모리와 실제 메모리 저장소 모두에 데이터를 업데이터 하는 정책을 의미한다.
- 장점
  - 캐시와 메모리에 업데이트를 같이 하여, 캐시의 일관성을 유지할 수 있다.
- 단점
  - 주기억장치 또는 보조기억장치는 캐시 메모리보다 데이터 처리 속도가 느리기 때문에 CPU가 대기하는 시간이 필요하여 성능이 떨어진다.
  - 위와 같은 문제를 해결하기 위해 버퍼를 이용한다.

### 다중 쓰기(Write Back)

- 쓰기가 발생했을 때 새로운 값은 캐시 내의 블록에만 쓰고, 후에 캐시에서 삭제되면 데이터를 주기억장치에도 저장하는 정책을 의미한다.
- 장점
  - 즉시 쓰기(Write Through)보다 처리 속도가 빠르다.
- 단점
  - 구현이 어려우며, 캐시 메모리에만 업데이트하기 때문에 캐시 일관성 유지가 힘들다.